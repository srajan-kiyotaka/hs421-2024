{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models of Binary Dependent Variable \n",
    "Lecture notes by\n",
    "_Sunil Paul_\n",
    "\n",
    "- The materials in this notebook is prepared using Scott Long's \"Regression models for categorical and Limited dependent variables\".The figure posted in the notes are sourced from this book.\n",
    "- Kindly refrain from circulating this note outside the class, as I haven't included citations for all the textbooks/paper I referenced in the preparation of these notes.\n",
    "- Also see Diebold, F.X. (2019), Econometric Data Science: A Predictive Modeling Approach, Department of Economics, University of Pennsylvania, http://www.ssc.upenn.edu/~fdiebold/Textbooks.html.”-\n",
    "\n",
    "\n",
    "Binary dependent variable models are utilized when outcomes or responses are discrete, typically taking values of 1 or 0. Examples include decisions such as whether to take a job or not, whether to pursue higher studies, and similar scenarios. These models are particularly suited for situations where the outcome can be categorized into two distinct groups.\n",
    "\n",
    "## The Linear Probability Model (LPM) \n",
    "When employing Ordinary Least Squares (OLS) to estimate parameters in the context of a binary or dummy variable as the dependent variable, the resultant regression is referred to as the Linear Probability Model (LPM). In the LPM, the dependent variable is binary, signifying outcomes such as the presence or absence of a particular event. For instance, this could encompass scenarios where the event of interest either happens or does not occur. \n",
    "\n",
    "It can be expressed as $$y=X\\beta +\\varepsilon,$$ where $E(y|X)=X\\beta$. \n",
    "For instance, suppose we aim to investigate the determinants of whether a woman is engaged in paid employment or not. In this case, the Linear Probability Model (LPM) can be specifically formulated as follows:  $$y_i=\\beta_{0}+\\beta_{1}x_{1i}+...+\\beta_{k}x_{ki},$$ where, $\\beta$s are the parameters of interest and $x_i$s are the independent variables and  $y_{i}$ is defined as:$$\n",
    "y_{i}=\\left\\{ \n",
    "\\begin{array}{l}\n",
    "1-\\text{if  ith woman is in paid employment} \\\\ \n",
    "0-\\text{otherwise}\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "### Why it is linear probability model?\n",
    "To comprehend why this model is termed the Linear Probability Model (LPM), it's essential to delve into the concepts of unconditional and conditional expectations of the dependent variable $y_{i}$. \n",
    "The unconditional expectation of $y_{i}$, denoted as $E(y_{i})$, is computed by summing the probabilities of $y_{i}$ taking the value 1 or 0, weighted by their respective occurrences. Mathematically, it is expressed as:\n",
    "\n",
    "\n",
    "$$E(y_{i})=[1\\times P(y_{i}=1)]+[0\\times P(y_{i}=0)]=P(y_{i}=1).$$ Similarly, the conditional expectation of $y_{i}$ given $x_{i}$, denoted as $E(y_{i}|x_{i})$, involves calculating the probabilities of $y_{i}$ being 1 or 0 given the values of $x_{i}$, and summing them up. $$E(y_{i}|x_{i})=[1\\times P(y_{i}=1|x_{i})]+[0\\times P(y_{i}=0|x_{i})]=P(y_{i}=1|x_{i})$$. Hence, $E(y_{i}|x_{i})=\\alpha +\\beta x_{i}=P(y_{i}=1|x_{i})=P_{i}$, where $\\beta$ represents the expected change in the probability of being in paid employment due to a one-unit change in $x_{i}$.\n",
    "\n",
    "In essence, $E(y_{i}|x_{i})$ provides the probability of the occurrence of the event of interest, given the specific conditions or characteristics denoted by $x_{i}$. In the context of analyzing whether a woman is engaged in paid employment, $E(y_{i}|x_{i})$ signifies the likelihood of being in paid employment based on the particular attributes represented by $x_{i}$. Therefore, the name LPM\n",
    "\n",
    "In matrix notation, the Linear Probability Model (LPM) can be represented as $E(y|X)=X\\beta =P$, where $y$ is a vector containing $n$ observations, and $X$ is the matrix of independent variables as defined in the previously circulated notes. Each element $\\beta_{k}$ in the coefficient vector $\\beta$ quantifies the expected change in the probability of the event occurring due to a one-unit change in the corresponding independent variable $x_{k}$, with all other variables held constant.\n",
    "\n",
    "A notable characteristic of the LPM is that a unit change in any independent variable $x_{k}$ consistently leads to the same change in the probability, regardless of the specific variable being altered. This uniform relationship between changes in the independent variables and alterations in the probability of the event occurring gives rise to the term 'Linear Probability Model'\n",
    "\n",
    "### Limitations of LPM\n",
    "However, the LPM has its limitations. One significant issue pertains to the properties of errors. The errors $\\varepsilon _{i}$ can take two values based on the outcome $y_{i}$, leading to non-normal distributions and heteroskedasticity. i.e:\n",
    "$$\\varepsilon _{i}=\\left\\{ \n",
    "\\begin{array}{l}\n",
    "1-X\\beta \\text{ if }y_{i}=1\\text{ with probability }P_{i} \\\\ \n",
    "-X\\beta \\text{ if }y_{i}=0\\text{ with probability }1-P_{i}\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "and the expection and variance of errors  can be derived as follows:\n",
    "$$E(\\varepsilon|X)=P(1-X\\beta )+(1-P)(-X\\beta\n",
    ") ,$$ substituting  $P=X\\beta $ we get\n",
    "\n",
    "$$E(\\varepsilon|X)=0,$$\n",
    "and\n",
    "$$\\begin{align*}\n",
    "\\text{var}(\\varepsilon|X) &= P(1-X\\beta)^{2} + (1-P)(-X\\beta)^{2} \\\\\n",
    "&= X\\beta (1-X\\beta)^{2} + (1-X\\beta)(-X\\beta)^{2} \\\\\n",
    "&= X\\beta (1-X\\beta)\n",
    "\\end{align*}.$$\n",
    "Since $\\text{var}(\\varepsilon|X) $ depends on X, its heteroschedastic. Hence, std. errors are biased and OLS is inefficient.\n",
    "\n",
    "Additionally, the interpretation of coefficients in the LPM may not always align with expectations, as the assumption of linearity in probability can be unrealistic in certain contexts. Also, $E(y|X)=X\\beta $ is interpreted as probabilities and in many cases $X\\beta $ lie outside the interval (0,1).\n",
    "\n",
    "#  Latent variable intrepretation \n",
    "\n",
    "In the scenario outlined above, consider an individual's labour force participation as the observed $y$. The variable $y$ can only be observed in two states: an individual is in the labour force or she is not. However, not all  in the labor force are there with the same certainty. One individual may be very close to the decision of leaving the labour force, while another individual could be very firm on her decision. In both cases, we observe the same $y=1$. The idea of a latent $y^*$ is that there is an underlying propensity to work that generates the observed state. While we cannot directly observe $y^*$ , at some point a change in $y^*$  results in a change in what we observe, namely whether an individual is in the labor force. To clarify, let's assume that individual $i$ has an expected wage (reservation wage), and they decide to enter employment if the market wage exceeds this expectation. Let $w_i$ represent the market wage, and $w_i^*$. Then the latent variable $y_i^*$ can be expressed as follows: \n",
    "$$y_{i}^{\\ast }=w_{i}- w_{i}^{\\ast }$$\n",
    "Given this,\n",
    "$$y_{i}=\\left\\{ \n",
    "\\begin{array}{l}\n",
    "1\\text{  if }y_{i}^{\\ast }>0 \\\\ \n",
    "0\\text{ if }y_{i}^{\\ast }\\leq 0\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "The latent variable $y^*$ is assumed to be linearly related to observed $x$s through the model as follows:\n",
    "$$y^*=X\\beta+\\varepsilon$$\n",
    "\n",
    "In general; $y_{i}^{\\ast }=X_{i}\\beta +\\varepsilon _{i}$ and\n",
    "\n",
    "$$y_{i}=\\left\\{ \n",
    "\\begin{array}{l}\n",
    "1\\ \\text{if }y_{i}^{\\ast }>\\tau \\\\ \n",
    "0\\ \\text{if }y_{i}^{\\ast }\\leq \\tau\n",
    "\\end{array}\n",
    "\\right. $$\n",
    "\n",
    "\n",
    "\n",
    "where$tau$ is the threshold or cutpoint. If $y^* \\le \\tau$, then $y=0$. If $y^*$ crosses the threshold $\\tau$(i.e., $y^*> \\tau$) then $y=1$.(We assume $\\tau=0$ for simplicity)\n",
    "\n",
    "![Example Image](fig1.png)\n",
    "\n",
    "The link between the latent $y^*$ is illustrated in the figure above for the model $y^*=\\beta_0+\\beta_1x+\\varepsilon$. $y^*$ is on the vertical axis, with the threshold $\\tau$ indicated by a horizontal dashed line. The distribution of $y^*$ shown by a bell shaped curve. When $y^*$ is larger than $\\tau$ indicated by the shaded region, we observe $y=1$.\n",
    "\n",
    "We cannot use Ordinary Least Squares (OLS) regression because the dependent variable is unobserved. Instead, we must use Maximum Likelihood Estimation (MLE), which requires specific assumptions about the errors. For example, in the Probit model, we assume the errors are normally distributed, while in the Logit model, we assume the errors follow a logistic distribution.\n",
    "\n",
    "## Probit  and Logit Model\n",
    "Distributional assumptions regarding $\\varepsilon$. However since $y^*$ is unobserved, we cannot estimate the variance of the errors. Instead, we need to assume some values for the variance of errors. We assume the $E(\\varepsilon|X)=0$ and if  we assume $var(\\varepsilon|X)=1$ it is a probit model and if we assume $var(\\varepsilon|X)=\\frac{\\pi^2}{3}\\approx3.29$ then it is a probit model. Probit model assumes standrard normal distribution for the errors and the pdf and cdf are given as follows:\n",
    "The pdf of errors in a probit model is:$$\\phi (\\varepsilon )=\\frac{1}{\\sqrt{2\\pi }}\\exp\n",
    "\\left( -\\frac{\\varepsilon ^{2}}{2}\\right) $$\n",
    "\n",
    "and the cdf :$$\\Phi (\\varepsilon )=\\int\\limits_{-\\infty }^{\\varepsilon }%\n",
    "\\frac{1}{\\sqrt{2\\pi }}\\exp \\left( \\frac{z_{i}^{2}}{2}\\right) dz_{i}.$$\n",
    "\n",
    "\n",
    "\n",
    "In the logit model errors are logistically distributed. The pdf of the error distribution is given below:\n",
    "$$\\lambda (\\varepsilon )=\\frac{\\exp (\\varepsilon )}{[1+\\exp\n",
    "(\\varepsilon )]^{2}}$$\n",
    "\n",
    "and the cdf:$$\\Lambda (\\varepsilon ) =\\frac{\\exp (\\varepsilon )}{1+\\exp (\\varepsilon )}$$\n",
    "\n",
    "Under the distributional assumptions, we posit that the latent variable$y^*$  follows a normal or logistic distribution, while its realization $y$ does not. However, we can infer the probability $P(y|X=1)$ using the error distribution. Specifically, when $y^*>0$, the probability $P(y|X=1)$ can be derived from the known probability density function (pdf) of $y^*>0$ determined by the error distribution. This is illustrated in the figure below. Given $\\varepsilon$ is distributed either logistically or normaaly around $E(y^*|x)=\\beta_0+\\beta_1x$. Values of $y=1$ are observed for the shaded portion of error distribution above $\\tau$\n",
    "\n",
    "![Example Image](fig2.png)\n",
    "\n",
    "The error distribution given in the figure above can be presented below to clarify the link. Since $y=1$ when $y^*>0,$ (Panel A in the figure)  $${P}(y=1|X)=P(y^{\\ast }>0|X)$$\n",
    "\n",
    "![Example Image](fig3.png)\n",
    "\n",
    "Substituting $y^{\\ast} = X\\beta + \\varepsilon$\n",
    " if follows that\n",
    "$${P}(y=1|X)=P(X\\beta+\\varepsilon>0|X)$$\n",
    "Substracting $X\\beta$ from each side of the inequality shifts the x-axis as shown in Panel B. Then $${P}(y=1|X)=P(\\varepsilon >-X\\beta |X)$$\n",
    " Since cdf's express the probability of a variable being less than some value we must change the direction of the inequality. The normal and logistic distributions are symmetric, which means that the shaded area of the distribution greater than $-X\\beta$ in Panel B equals the shaded area less than $X\\beta$ in panel C. Consequently.\n",
    " $${P}(y=1|X)=P(\\varepsilon \\le X\\beta |X)$$\n",
    "\n",
    "This is simply the cdf of the error distribution evaluated at $X\\beta$. Accordingly, $$P(y=1|X)=F(X\\beta)$$\n",
    "\n",
    "As each $x_i$ in $X$ increases by one unit, the argument of $F$ increases by $\\beta_i$ units, and $F(\\cdot) \\in [0,1]$. The plot below illustrates the probit/logit regression. Panel A shows the error distribution of nine values of $x$. The region of the distribution where $y^* > \\tau$ corresponds to $P(y=1|x)$. At $x_1$, only a small portion of the tail of the curve crosses the threshold in panel A, resulting in a small value for $P(y=1|x)$ in the panel B below. As we move to $x_2$, the error distribution shifts up slightly.\n",
    "\n",
    "![Example Image](fig4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Higher values of $X_{i}\\beta $ indicate a greater likelihood of the event occurring. Care must be taken in interpreting estimation results in this context. A one-unit change in $X_{i}$ corresponds to a $\\beta $ change in the z-score of Y. The estimated curve takes the form of an S-shaped cumulative normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML estimation\n",
    "Consier a simple regression with one independent variable $x_i$ for simplicity. To specify the liklihood equation, let $p_i=P(y_i=1|x_i)$ and $1-p_i=P(y=0|xi)$. Then, the likelihood is the product of $p_i$ for all observations for where $y=1$ multiplied by the product of $1-_i$ for which $y=0$: $$L(\\beta_0, \\beta_1|y_,x_i)=\\prod\\limits_{i=1}^{n1}p_{i}\\prod\\limits_{i=1}^{n2}1-p_{i},$$ where $n1$ is the number of observations where $y_{i}=1$ and $n2$ is the number of observations where $y_{i}=0$.\n",
    "\n",
    "This can be expressed as substituting the $pi= P(y=1|X)=F(X\\beta)$ $$L(\\beta_0, \\beta_1|y_,x_i)=\\prod\\limits_{i=1}^{n1}F(\\beta_0+\\beta_1x_i)\\prod\\limits_{i=1}^{n2}[1-F(\\beta_0+\\beta_1x_i )]$$. \n",
    "\n",
    "Taking the logarithm, we obtain the log-likelihood function $$\\ln (L)=\\sum\\limits_{i=1}^{n1}\\ln [F(\\beta_0+\\beta_1x_i )]\\sum\\limits_{i=1}^{n2}\\ln [1-F(\\beta_0+\\beta_1x_i)]$$. Maximizing $\\ln (L)$ with respect to $\\beta $ gives the maximum likelihood (ML) estimator of $\\beta $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "For each observation of independent variables, we can estimate the predicted probabilities. These predicted probabilities can be summarized as a range, from the minimum to the maximum. The predicted probability for the $i$th individual is denoted as $\\widehat{P}(y_{i}=1|X)=F(X\\widehat{\\beta })$. For positive $\\beta$ values, $F(X\\widehat{\\beta })$ yields the minimum if we take the lower extreme of $x_{i}$. Conversely, if $\\beta $ is negative, the lower extreme value of a variable will return the maximum probability. It's important to exercise caution with extreme values (outliers) as they can lead to misleading interpretations. In the presence of outliers, it may be more appropriate to consider the 5th and 95th quantiles instead of the minimum and maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Interpretation:Example(Probit)__\n",
    "- $y_{i}=1$ if the ith individual travels by own vehicles, 0=otherwise\n",
    "- $x_{i}=$ Difference in the travel time between public transport and own\n",
    "vehicle( in minutes)\n",
    "- $p_{i}=\\Phi (X_{i}\\beta )=\\Phi (\\alpha +\\beta x_{i})=\\Phi (Z)$\n",
    "- $\\widehat{p}_{i}=\\Phi (\\underset{(-0.161)}{-0.0644}+\\underset{(2.95)}{0.0299}x_{i})$\n",
    "- If $x_{i}=20:\\Phi (0.54),$ hence $P(z\\leq 0.54)=0.7054$\n",
    "- If $x_{i}=5:\\Phi (0.085),P(z\\leq 0.085)=0.53387$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation:Marginal effects \n",
    "\n",
    "\n",
    "In bivariate linear regression, if the coefficient on $x_{1}$ is $\\beta_{1}$, then a 1-unit increase in $x_{1}$ increases $y$ by $\\beta_{1}$. In a probit model, a 1-unit increase in $x_{1}$ increases the z-score of $P(y=1)$ by $\\beta_{1}$. An increase in $x_{1}$ has a constant effect on $y^{\\ast }$, but this doesn't translate into a constant effect on the original $y$ or its probability. For a logit/probit model, $p=P(y=1|x_{1i})=F(\\beta_0+\\beta_1x_{1i} )$, where $F$ is either the cumulative distribution function $\\Phi $ for a normal distribution or the cumulative distribution function $\\Lambda $ for the logistic distribution. The marginal effect of $x_{k}$, the partial change in the probability due to a change in $x_{k}$, is given by$$$\\frac{\\partial p_{i}}{\\partial x_{k}}=\\frac{\\partial F(X_{i}\\beta )}{\n",
    "\\partial x_{k}}=\\frac{dF(X\\beta )}{dX\\beta }\\frac{\\partial X\\beta }{\n",
    "\\partial x_{k}}=f(X\\beta )\\beta _{k},$$ where $f$ is the probability density function. \n",
    "\n",
    "In a probit model, $\\frac{\\partial p{i}}{\\partial x_{k}}=\\phi (\\beta_0+\\beta_1x_{1i}+...+\\beta_{ki} )\\beta_{k}$. Since $f(X\\beta )$ is always positive, the sign of $\\widehat{\\beta}_{k}$ indicates the direction of the relation between $x_{k}$ and $p_{i}$. The magnitude of the change in the probability given a change in $x_{k}$ is determined by the magnitude of $\\widehat{\\beta }_{k}$ and $f(X\\beta )$. Consequently, $\\frac{\\partial p_{i}}{\\partial x_{k}}$ varies with $x_{k}$ and values of other independent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, $$\\widehat{p}{i}=\\Phi (\\underset{(-0.161)}{-0.0644}+\\underset{(2.95)}{0.0299}x{i})$$. $$\\frac{\\partial \\widehat{p}{i}}{\\partial x{i}}=(\\widehat{\\beta })\\times \\phi (\\widehat{\\alpha }+\\widehat{\\beta }x_{i})=(\\widehat{\\beta })\\times \\phi (z)=(0.0299)\\times \\underset{\\text{Changes with }x_{i}}{\\underbrace{\\phi (-0.0644+0.0299x_{i})}}.$$ If $x_{i}=20$, $\\frac{\\partial \\widehat{p}{i}}{\\partial x{i}}=0.0299\\times \\phi (0.54)=0.0104$ where $\\phi (0.54)=0.3448$. \n",
    "\n",
    "Since marginal effects depend on the level of all variables, we need to summarize both. There are two prominent methods. One method is to compute the average over all observations: Mean$\\frac{\\partial \\widehat{p}_{i}}{\\partial x_{ik}}=\\frac{1}{N}\\sum\\limits_{i=1}^{N}f(X\\beta )\\beta_{k}$. Another method is to compute the marginal effect at the mean of the independent variables: $\\frac{\\partial \\widehat{p}_{i}}{\\partial x_{ik}}=f(\\overline{X}\\beta )\\beta_{k}$. Both methods give identical results most of the time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The marginal effect of a dummy variable is $\\widehat{p}_{i}^{1}-\\widehat{p}_{i}^{0}$. \n",
    "\n",
    "Exactly the same issues arise with the logit model as with the probit model (but the formulas differ). The predicted probability is $\\widehat{P}(y_{i}=1|X)=F(X\\widehat{\\beta })$ where $F=\\Lambda $. The marginal effects are $\\frac{\\partial \\widehat{p}_{i}}{\\partial x_{ik}}=\\frac{\\partial \\Lambda (X\\widehat{\\beta })}{\\partial x_{ik}}=\\lambda (X\\beta )\\beta_{k}$, which is equal to $\\frac{\\exp (X\\beta )}{[1+\\exp (X\\beta )]^{2}}\\beta _{k}$. \n",
    "#### Logit in terms of odds: \n",
    "Logit was derived from a latent variable perspective so far. There is an alternative derivation based on the odds ratio. If some event occurs with probability $p=P(y=1|X)$, then the odds of it happening are $$O(p)=p/(1-p)$$. Odds indicate how often something happens relative to how often it doesn't happen: $$p=0\\Rightarrow O(p)=0$$, $$p=0.25\\Rightarrow O(p)=1/3$$ (odds are 1-to-3 against), $$p=0.75\\Rightarrow O(p)=3/1$$ (odds are 3-to-1 in favor)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For original $y=(0,1)$, $y$ as a probability $(0$ to $1)$, and odds of $y=(0$ to $\\infty )$, the log odds of $y=(-\\infty $ to $\\infty )$. $$p_{i}=P(y_{i}=1|X)=\\frac{\\exp (X\\beta )}{1+\\exp (X\\beta )},\\Rightarrow (0 \\text{ to} 1)$$. $$1-p_{i}=1-P(y_{i}=1|X)=\\frac{1}{1+\\exp (X\\beta )}.$$ In particular, the odds are $$\\Omega (X)=\\frac{p_{i}}{1-p_{i}}=\\exp (X\\beta )\\Rightarrow (0 \\text{ to }\\infty ).$$ The log odds, known as logit, are $$\\ln \\Omega (X)=\\ln \\left( \\frac{p_{i}}{1-p_{i}}\\right) =X\\beta \\Rightarrow (-\\infty \\text{ to }\\infty).$$  Since logit is the log of odds, logistic slope coefficients can be interpreted as the effect of a unit of change in the $X$ variable on the predicted logits with the other variables in the model held constant: $\\ln \\Omega (X)=X\\beta $. $\\frac{\\partial \\ln \\Omega (X)}{\\partial x_{k}}=\\beta _{k}$. We can also drive the impact on a change in one variable on the odds ratio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example ( taken from Gujarati)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data used here is a random sample of 1,196 US males.1.\n",
    "The variables used in the analysis are as follows:\n",
    "_Smoker_ = 1 for smokers and 0 for nonsmokers\n",
    "_Age_ = age in years\n",
    "_Education_ = number of years of schooling\n",
    "_Income_ = family income\n",
    "_Pcigs_ = price of cigarettes in individual states in 1979\n",
    "### Linear Probability model\n",
    "\n",
    "The Model to be estimated:\n",
    "$$y_i=\\beta_0+ \\beta_1 Age_i + \\beta_2 Educ_i + \\beta_3 Income_i+ \\beta_4 Pcigs+u_i$$\n",
    "where \n",
    "$y_i$=1 if the individual is smoker and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please intsall these packages if not installed in your syestem\n",
    "library(huxtable)\n",
    "library(knitr)\n",
    "library(tidyverse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "|    term     | estimate | std.error | statistic | p.value |\n",
       "|:-----------:|:--------:|:---------:|:---------:|:-------:|\n",
       "| (Intercept) |   1.12   |   0.19    |   5.96    |  0.00   |\n",
       "|     age     |   0.00   |   0.00    |   -5.70   |  0.00   |\n",
       "|    educ     |  -0.02   |   0.00    |   -4.47   |  0.00   |\n",
       "|   income    |   0.00   |   0.00    |   0.63    |  0.53   |\n",
       "|   pcigs79   |  -0.01   |   0.00    |   -1.80   |  0.07   |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- read.csv(\"logit_smoke.csv\", sep=\",\")\n",
    "lpm <- lm(smoker~age+educ+income+pcigs79, data=data)\n",
    "kable(tidy(lpm), digits=2, align='c', caption=\n",
    "        \"LPM model of to smoke or not to smoke.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that all the variables, except income, are individually statistically significant\n",
    "at least at the 10% level of significance.\n",
    "\n",
    "The interpretation of the regression coefficients is as follows. If we hold all other variables constant, the probability of smoking decreases at the rate of $\\approx$0.005 as a person ages, probably due to the adverse impact of smoking on health. Likewise, other things being equal, an increase\n",
    "in schooling by one year decreases the probability of smoking by 0.02. Similarly, if\n",
    "the price of cigarettes goes up by a dollar, the probability of smoking decreases by\n",
    " $\\approx$ 0.005, holding all other variables constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "0.0387696934677072"
      ],
      "text/latex": [
       "0.0387696934677072"
      ],
      "text/markdown": [
       "0.0387696934677072"
      ],
      "text/plain": [
       "[1] 0.03876969"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(lpm)$r.squared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The R2 value of  $\\approx$ 0.038 seems very low,\n",
    "but one should not attach much importance to this because the dependent variable is\n",
    "nominal, taking only values of 1 and zero. The LPM  is not the preferred choice for modeling indicator variables for the reasons I mentioned in the class. Let us estimate it via Probit and Logit models. use **glm()** function to estimate the models.\n",
    "### Logit model\n",
    "The results of the logit model is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "|    term     | estimate | std.error | statistic | p.value |\n",
       "|:-----------:|:--------:|:---------:|:---------:|:-------:|\n",
       "| (Intercept) |  2.7451  |  0.8292   |  3.3105   | 0.0009  |\n",
       "|     age     | -0.0209  |  0.0037   |  -5.5773  | 0.0000  |\n",
       "|    educ     | -0.0910  |  0.0207   |  -4.4021  | 0.0000  |\n",
       "|   income    |  0.0000  |  0.0000   |  0.6583   | 0.5104  |\n",
       "|   pcigs79   | -0.0223  |  0.0125   |  -1.7895  | 0.0735  |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_logit<- glm(smoker ~ age + educ + income + pcigs79, family=binomial(link=\"logit\"), \n",
    "                   data=data)\n",
    "kable(tidy(s_logit), digits=4, align='c', caption=\n",
    "        \" 1. Logit model of to smoke or not to smoke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of the logit model in  is as follows: each slope coefficient\n",
    "shows how the log of the odds in favor of smoking changes as the value of the X\n",
    "variable changes by a unit \n",
    "Let us examine these results. The variables age and education are highly statistically significant and have the expected signs. As age increases, the value of the logit\n",
    "decreases, perhaps due to health concerns – that is, as people age, they are less likely\n",
    "to smoke. Likewise, more educated people are less likely to smoke, perhaps due to\n",
    "the ill effects of smoking. The price of cigarettes has the expected negative sign and\n",
    "is significant at about the 7% level. Keeping all other variables constant, the higher the price of cigarettes,\n",
    "the lower is the probability of smoking. Income has no statistically visible impact on\n",
    "smoking, perhaps because expenditure on cigarettes may be a small proportion of\n",
    "family income.\n",
    "The interpretation of the various coefficients is as follows: holding other variables\n",
    "constant, if, for example, education increases by one year, the average logit value goes\n",
    "down by$\\approx$0.09, that is, the log of odds in favor of smoking goes down by about 0.09.\n",
    "Other coefficients are interpreted similarly.\n",
    "#### Marginal effects\n",
    "In the LPM the slope coefficient measures the marginal effect of a unit change in\n",
    "the explanatory variable on the probability of smoking, holding other variables\n",
    "constant. This is not the case with the logit model, for the marginal effect of a unit change in the explanatory variable not only depends on the coefficient of\n",
    "that variable but also on the level of probability from which the change is measured. But the latter depends on the values of all the explanatory variables in the\n",
    "model. You can use functions from *mfx* packge to get the marginal effects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    " library(\"mfx\")\n",
    "mlogit <-logitmfx(smoker ~ age + educ + income + pcigs79,data=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "logitmfx(formula = smoker ~ age + educ + income + pcigs79, data = data)\n",
       "\n",
       "Marginal Effects:\n",
       "              dF/dx   Std. Err.       z     P>|z|    \n",
       "age     -4.8903e-03  8.7359e-04 -5.5979 2.170e-08 ***\n",
       "educ    -2.1334e-02  4.8365e-03 -4.4111 1.029e-05 ***\n",
       "income   1.1069e-06  1.6814e-06  0.6583   0.51033    \n",
       "pcigs79 -5.2340e-03  2.9242e-03 -1.7899   0.07347 .  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mlogit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interpretation of these marginal effects is as follows: Keeping all other variables constant, if mean\n",
    "age increases by a year, it reduces the probability of smoking approximately by 0.005.\n",
    "Likewise, if mean education increases by a year, the probability of smoking goes\n",
    "down by about 0.021, Keeping all other variables constant. A unit changes in mean income has no statistically significant impact on the rate of change in the probability of smoking. A unit\n",
    "increase in the mean price of a pack of cigarettes reduces the probability of smoking\n",
    "by about .005, which is significant at about the 7% level\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The language of the odds ratio (OR)\n",
    "Instead of expressing logits in terms of probabilities, we can express them in terms\n",
    "of the odds ratios (OR). For our example, using Stata 12, we obtained the odds ratios\n",
    "shown in Table below gives the odds ratios, their standard errors, and their z values (instead\n",
    "of the t statistic)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "logitor(formula = smoker ~ age + educ + income + pcigs79, data = data)\n",
       "\n",
       "Odds Ratio:\n",
       "           OddsRatio    Std. Err.       z         P>|z|    \n",
       "age     0.9793627211 0.0036617871 -5.5773 0.00000002443 ***\n",
       "educ    0.9130425607 0.0188687267 -4.4021 0.00001072081 ***\n",
       "income  1.0000047202 0.0000071705  0.6583       0.51036    \n",
       "pcigs79 0.9779283903 0.0121969923 -1.7895       0.07354 .  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "options(scipen=999)\n",
    "orlogit <-logitor(smoker ~ age + educ + income + pcigs79,data=data)\n",
    "(orlogit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " These odds ratios are obtained by exponentiating the coefficients given in Table 1. For example,\n",
    "the coefficient of age in Table 1 is –0.020853. Therefore, we compute $e^{–0.020853}$ =\n",
    "0.9793627, which gives the value of the odds ratio.To interpret the odds ratios, keep in mind that an OR greater than 1 indicates an increased chance that an event (smoking) will occur rather than it not occurring. An odds ratio of less than 1 suggests a decreased chance of the event occurring versus\n",
    "not occurring. An OR of 1 means that the chances of an event occurring or not\n",
    "occurring are even. Another property of the odds ratio is that it is invariant with\n",
    "respect to the ordering of the variables. Thus, ORa/ORb = 1/(ORb/ORa), where a and\n",
    "b are two events. You will observe that the odds ratios for age and education\n",
    "are less than one, suggesting that the probability of smoking decreases with respect\n",
    "to these variables; not a surprising finding. More specifically, the odds of smoking\n",
    "decrease by about 2% as one ages, ceteris paribus. Similarly, an additional year of\n",
    "education reduces the odds of smoking by about 8.7%, ceteris paribus. The OR for income is not much different from 1. Th e OR for picgs79 also does not seem much\n",
    "different from one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probit Model\n",
    "\n",
    "As we have disussed in the class in  the probit model the error term has the normal distribution.Although the numerical values of the logit and probit coefficients are different, qualitatively the results are similar: the coefficients of age, education, and price of cigarettes are individually significant at least at the 10% level. The income coefficient,however, is not significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "|    term     | estimate | std.error | statistic | p.value |\n",
       "|:-----------:|:--------:|:---------:|:---------:|:-------:|\n",
       "| (Intercept) |   1.70   |   0.51    |   3.34    |  0.00   |\n",
       "|     age     |  -0.01   |   0.00    |   -5.70   |  0.00   |\n",
       "|    educ     |  -0.06   |   0.01    |   -4.46   |  0.00   |\n",
       "|   income    |   0.00   |   0.00    |   0.62    |  0.54   |\n",
       "|   pcigs79   |  -0.01   |   0.01    |   -1.79   |  0.07   |"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "s_probit <- glm(smoker ~ age + educ + income + pcigs79, family=binomial(link=\"probit\"), \n",
    "                   data=data)\n",
    "kable(tidy(s_probit), digits=2, align='c', caption=\n",
    "        \"Probit model of to smoke or not to smoke\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Marginal effects\n",
    "The interpretations are similar to logit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Call:\n",
       "probitmfx(formula = smoker ~ age + educ + income + pcigs79, data = data)\n",
       "\n",
       "Marginal Effects:\n",
       "                dF/dx     Std. Err.       z         P>|z|    \n",
       "age     -0.0049204438  0.0008628909 -5.7023 0.00000001182 ***\n",
       "educ    -0.0213399178  0.0047859798 -4.4588 0.00000824044 ***\n",
       "income   0.0000010324  0.0000016740  0.6168       0.53740    \n",
       "pcigs79 -0.0052348582  0.0029164385 -1.7949       0.07266 .  \n",
       "---\n",
       "Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mprobit <-probitmfx(smoker ~ age + educ + income + pcigs79,data=data)\n",
    "\n",
    "mprobit"
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/a84424be8fad7b4089241dd31e0c63c6"
  },
  "gist": {
   "data": {
    "description": "OneDrive/HS421/Notes/Untitled1.ipynb",
    "public": false
   },
   "id": "a84424be8fad7b4089241dd31e0c63c6"
  },
  "kernelspec": {
   "display_name": "R 4.2.2",
   "language": "R",
   "name": "ir42"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "164.992px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
