{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violations of Ideal Condition: Specification errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideal conditions Again\n",
    "\n",
    "1. The data-generating process (DGP) is:\n",
    "$$y=X\\beta+\\varepsilon$$\n",
    "$$\\varepsilon \\sim N(0,\\sigma^2 I)$$ and the fitted model matches it exactly.\n",
    "\n",
    "    1. The fitted model is correctly specified\n",
    "    2.  The disturbances are Gaussian\n",
    "    3.  The coefficients$(\\beta)$ are fixed\n",
    "    3. The relationship is linear\n",
    "    4. $Var(\\varepsilon)=\\sigma^2I$; variance of error are constant\n",
    "    5. $E(\\varepsilon_i\\varepsilon_j)=0;i\\ne j$ errors are uncorrelated\n",
    "2. $E(\\varepsilon X)=0 $  errors are  independent of $X$\n",
    "    1. $E(\\varepsilon|X)=0$\n",
    "    2. $Var(\\varepsilon|X)=\\sigma^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this section, we will briefly discuss  the consequences of deviations from some of the ideal conditions listed above. We will delve into point 2 in detail later in the course. As we previously discussed, violations of this point result in an endogeneity problem. Similarly, the assumption of no autocorrelation (IC 1 F.) will also be addressed later when we cover time series analysis. Notes for heteroschedasticity (violation of 1 E.) will be circulated seperately. In this we will discuss some common speci",
    "cation problems mainly i) omission of relevant variables, ii) inclusion of irrelevant variables and iii) Incorrect functional forms. These are related to assumption 1.A and D. In additon, a briefdiscussion of  the problem of multicollinerariy and errors in measurement of dependent and independent varibles are also included in this note "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "## Specification errors (IC 1.A)\n",
    "\n",
    "### Omission of Relevent variables: Ommitted variable Bias\n",
    "\n",
    "Let the true model is given by $\\ddot{y_i}=\\beta _{1}\\ddot{x}_{1i}+\\beta _{2}\\ddot{x}_{2i}+\\varepsilon$\n",
    "(variables are expressed in deviation form ie $\\ddot{y}=y-\\bar{y}$.) and we estimate the model $\\ddot{y_i}=\\beta _{1}\\ddot{x}_{1i}+u$ (Misspecified model). (we won't use the subscript 'i' hereafter and $y$ as $y_i$).(I might have used different notation in the class. The deviation form is easy to work with. It can be easily derived. Remeber the intercept can be defined as $\\beta_0=\\bar{y}-\\beta_1 \\bar{x}_1 $ given the regression $y_i=\\beta_0-\\beta_1x_{1i}+ \\varepsilon_i$. Now substituting the expression of  $\\beta_0$ and rearranging we get $(y_i-\\bar{y})=\\beta_1(x_{1i}-\\bar{x}_1)+ \\varepsilon_i$. This can be easily genaralised to a multiple regression model.)\n",
    "\n",
    "Let the estimate of $\\beta _{1}$\n",
    "in the misspecified model equal to $\\widehat{\\beta }_{1}$ which is equal to $%\n",
    "\\frac{\\sum \\ddot{x}_{1}\\ddot{y}}{\\sum \\ddot{x}_{1}^{2}}$. Now substitute the true model into the\n",
    "expression of $\\widehat{\\beta }_{1}$. Then we get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\widehat{\\beta }_{1} =\\frac{\\sum \\ddot{x}_{1}(\\beta _{1}\\ddot{x}_{1}+\\beta _{2}\\ddot{x}_{2}+\\varepsilon)\n",
    "}{\\Sigma \\ddot{x}_{1}^{2}}=\\beta _{1}+\\beta _{2}\\frac{\\Sigma \\ddot{x}_{1}\\ddot{x}_{2}}{\\sum \\ddot{x}_{1}^{2}}\n",
    "+\\frac{\\sum \\ddot{x}_{1}\\varepsilon}{\\sum \\ddot{x}_{1}^{2}}$$ since $E(\\Sigma \\ddot{x}_{1}\\varepsilon)=0$ we get $$E(\n",
    "\\widehat{\\beta }_{1}) =\\beta _{1}+\\beta _{2}b_{21};$$where $b_{21}=\\frac{\\sum \\ddot{x}_{1}\\ddot{x}_{2}}{\\sum \\ddot{x}_{1}^{2}}$ is the regression coefficient from a\n",
    "regression of $x_{2}$ on $x_{1.}$ Hence the coefficient from misspecifed\n",
    "model will be biased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Bias**=(coefficient of the excluded variable) X (regression coefficient in\n",
    "a regression of the excluded variable on the included variables).\n",
    "\n",
    "From the definition of bias we can also infer that the $\\widehat{\\beta }_{1}$\n",
    "will be unbiased if $\\beta _{2}=0$ (if the omitted variable has no effect on\n",
    "Y) and $b_{21}=0$ (i.e. if $x_{2}$ has no effect on $x_{1}$ i.e. when $x_{2}$\n",
    "and $x_{1}$are uncorrelated).\n",
    "\n",
    "Similarly as the error term captures the influence of omitted variable the variance would be incorrectly estimated as a result the varriance of coefiifients would also be biased.\n",
    " \n",
    "**General case:**\n",
    "\n",
    "If we have k-1 included variable and k th variable is omitted then\n",
    "\n",
    "$E(\\widehat{\\beta }_{i})=\\beta _{i}+b_{ki}\\beta _{k}$ , $i=1.2....,k-1$\n",
    "where $b_{ki}$ is the regression coefficient of $x_{i}$ in the auxiliary\n",
    "regression of $x_{k}$ on $x_{1,}x_{2},...x_{k-1}.$\n",
    "\n",
    "If among k explanatory variable on k1 are included and remaining variable\n",
    "are excluded(k-k1) then the above expression can be written as,\n",
    "\n",
    "$E(\\widehat{\\beta }_{i})=\\beta _{i}+\\sum_{j=k_{1}+1}^{k}b_{ji}\\beta _{k}$ , $%\n",
    "i=1,2,...,k-1$ where $b_{ji}$ is the regression coefficient of the included\n",
    "variable in a regression of the j the omitted variable on all included\n",
    "variable.\n",
    "\n",
    "To summarize, the consequences of omitting a relevant variable are:\n",
    "\n",
    " - If the omitted variable is correlated with the included variable, then the estimates of the constant and slope coefficients are biased and inconsistent\n",
    "\n",
    "- If the omitted variable is not correlated with the included variables, then the slope coefficient is unbiased.\n",
    "\n",
    "-  The disturbance variance $\\sigma ^{2}$is incorrectly estimated.\n",
    "\n",
    "-  The estimated variance of the slope coefficient is a biased estimator of the variance of the true estimator from the fully-specified model.\n",
    "\n",
    "-  As a result, confidence intervals and hypothesis testing procedures are likely to give misleading results about the significance of parameters.\n",
    "\n",
    "Generaly the bias due to omitting relevent variable is known as \"Omitted variable bias\" i.e. If an independent  is correlated with a variable that has been omitted from the analysis  and that determines, in part, the\n",
    "dependent variable , then the OLS estimator will have omitted variable bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for Omitted variable bias: Is coffe good for health? (reproduced from Stock and Watson pp 214)\n",
    " \n",
    "A study published in the Medicine (Gunter, Murphy, Cross, et al. 2017) Annals of Internal suggested that drinking coffee is linked to a lower risk of disease or death. This study was based on examining 521,330 participants for a mean period of 16 years in 10 European countries. From this sample group, 41,693 deaths were recorded during this period. Another recent study published in The Journal of the American Medical Association (Loftfield, Cornelis, Caporaso, et al. 2018) investigated the link between heavy intake of coffee and risk of mortality. It suggested that drinking six–seven cups of coffee per day was associated with a 16% lower risk of death. This study attracted substantial attention in the U.K. press, with articles bearing headlines such as “Six coffees a day could save your life” and “Have another cup of coffee! Six cups a day could decrease your risk of early death by up to 16%, National Cancer Institute study finds.” Are these headlines accurate? Perhaps not. While they suggest a causal relationship between coffee and life expectancy, there is the potential for omitted variable bias to influence the relationship being established. Reviews of this study, including those by the United Kingdom’s National Health Service (NHS) and the BMJ, note that some people may opt not to drink coffee if they know they have an illness already.\n",
    "\n",
    "Similarly, coffee can be considered as a surrogate endpoint for factors that affect health—income, education, or deprivation—that may confound the observed beneficial associations and introduce errors. According to a paper published in BMJ (Poole, Kennedy, Roderick, et al. 2017), randomized controlled trials (RCTs), or randomized controlled experiments, allow for many of these errors to be removed. In this case, removing the ability of people to select if they should drink coffee and how much they should consume would remove any omitted variable bias arising from differences in income or in expectations about\n",
    "health among coffee drinkers and non-coffee drinkers. Sometimes, however, there may be neither a genuine relationship that an RCT could detect, nor even an omitted variable responsible for the relationship. The website “Spurious Correlations” details many such examples. For instance, the per capita consumption of mozzarella cheese over time  hows a strong, and coincidental, relationship with the award of civil engineering doctorates. Be careful when interpreting the results of regressions!\n",
    "\n",
    "- See the studies by Gunter, Murphy, Cross, et al., “Coffee Drinking and Mortality in 10 European Countries: A Multinational Cohort Study,” Annals of Internal Medicine, http://annals.org, July 11, 2017.\n",
    "\n",
    "- Read the paper on “Association of Coffee Drinking With Mortality by Genetic Variation in Caffeine Metabolism,Findings From the UK Biobank,” by See Loftfield, Cornelis, Caporaso, et al., published in JAMA Internal Medicine,July 2, 2018.\n",
    "\n",
    "- Laura Donnelly, “Six Coffees a Day Could save YourLife,” The Telegraph, July 2, 2018, https://www.telegraph .co.uk; and Mary Kekatos, “Have Another Cup of Coffee! Six Cups a Day Could Decrease Your Risk of Early Death by up to 16%, National Cancer Institute Study Finds,” TheDaily Mail, July 2, 2018.\n",
    "\n",
    "- For further reading, see “Another Study Finds Coffee Might Reduce Risk of Premature Death,” on the NHS website; and “Coffee Consumption and Health: Umbrella Review of Meta-analyses of Multiple Health Outcomes,”by Robin Poole, Oliver J Kennedy, Paul Roderick, Jonathan A. Fallowfield, Peter C Hayes, and Julie Parkes, published on the British Medical Journal (BMJ) website,October 16, 2017, http://dx.doi.org/10.1136/bmj.j5024.\n",
    "\n",
    "- on Spurious Correlations, http:// www.tylervigen.com/spurious-correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Including an Irrelevant Variable\n",
    "\n",
    "Similarly,the consequences of including a irrelevant variable are:\n",
    "\n",
    "- LS estimators ($\\widehat{\\beta })$ and their estimators of variance remain unbiased\n",
    "- LS estimators are not efficient i.e. there variance will be larger (given the included variable are not correlated with other independent variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dignostic Tests\n",
    "\n",
    "How to decide which variable to be included in the regression model or not?\n",
    "We can decide whether a variable to be included or excluded \n",
    "-  using economic theory;\n",
    "- Use F test.\n",
    "\n",
    "F test can be nested or non-nested .Both these test are discussed below:\n",
    "\n",
    "Unrestricted model:$y=\\beta _{0}+\\beta _{1}x_{1}+\\beta _{2}x_{2}+\\varepsilon$\n",
    "\n",
    "Restricted model :$y=\\beta _{0}+\\beta _{1}x_{1}+\\varepsilon$\n",
    "\n",
    "Weather to include or exclude $x_{2}$ can be done by constructing F test forthe hypothesis $\\ H_{0}:\\beta _{2}=0.$ This is an example of nested hypothesis. One of the model under consideration can be derived from another. \n",
    "\n",
    "### Solutions \n",
    "**when other observable variables are avilable** If you include another variable in your multiple regression, you will eliminate the possibility of omitted variable bias from excluding that variable, but the variance of the estimator of the coefficients of interest can increase. Here are some guidelines to help you decide whether to include an additional variable (Stock and Watson):\n",
    "-  Be specific about the coefficient or coefficients of interest.\n",
    "-  Use a-priori reasoning to identify the most important potential sources of omitted variable bias, leading to a base specification and some “questionable” variables.\n",
    "-  Test whether additional, “questionable” control variables have nonzero coefficients, and assess whether including a questionable control variable makesa meaningful change in the coefficient of interest.\n",
    "\n",
    "**when control variables are not available**\n",
    "- Use obserable data from other time period\n",
    "- Instrumental variable regression\n",
    "- Randomised Contralled Experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorrect Functional form (Functional Form Misspecification)\n",
    "\n",
    "Specification errors can happen due to wrong functional form.\n",
    "If the true population regression function is nonlinear but the estimated regression is\n",
    "linear, then this functional form misspecification makes the OLS estimator biased. This\n",
    "bias is a type of omitted variable bias, in which the omitted variables are the terms that\n",
    "reflect the missing nonlinear aspects of the regression function.\n",
    "\n",
    "Let the true functional form relating $Y$ to $X$ is given as follows\n",
    "\n",
    "True Model:$y=\\beta _{0}+\\beta _{1}x+\\beta _{2}x^{2}+\\varepsilon_1$\n",
    "\n",
    "But we estimate the following model instead;\n",
    "\n",
    "$Y=\\beta _{0}+\\beta _{1}x+\\varepsilon_1.$\n",
    "\n",
    "In this case we are omitting an explanatory variables obviously correlated\n",
    "with an included variable, leading to biased and inconsistent OLS estimators.\n",
    "\n",
    "We may also encouter different kind of  'functional form misspecification'\n",
    "\n",
    "Consider the following model for example;\n",
    "\n",
    "True model: $Y=$:$\\beta _{0}+\\beta _{1}\\ln (X)+\\varepsilon_1$\n",
    "\n",
    "Instead we estimate $Y=\\alpha _{0}+\\alpha _{1}X+\\varepsilon_2$\n",
    "\n",
    "#### Tests\n",
    "A general test for function form specification is suggested by Ramsey (1969).The test procedure of Ramsey is given below \n",
    "**Ramsey Regression Specification Error Test(RESET)**\n",
    "\n",
    "Assume that the true model is linear then no non-linear  functions of independent variable should be significant. The steps involved in RESET test is given below\n",
    "\n",
    "Step 1\n",
    "\n",
    "Estimate the model : $y=\\beta _{1}+\\beta _{2}x+...+\\beta _{k}x_{k}+\\varepsilon_1$ and\n",
    "obtain $\\widehat{y}$ and the restricted $RSSR$\n",
    "\n",
    "Step 2\n",
    "\n",
    "Estimate the extended model  as follows:\n",
    "\n",
    "$y=\\beta _{0}+\\beta _{1}x+...+\\beta _{k}x_{k}+\\delta _{1}\\widehat{y}%\n",
    "^{2}+...+\\delta _{q}\\widehat{y}^{q}+\\varepsilon_2$ and obtain the Unrestrieted SSR(USSR)\n",
    "\n",
    "Step 3\n",
    "\n",
    "In order to test \n",
    "\n",
    "$H_{0}:\\delta _{1}=...=\\delta _{q}=0$\n",
    "\n",
    "construct F test statistics as follows\n",
    "\n",
    "$$F=\\frac{RSSR_{r}-USSR/(q)}{USSR_{r}/(n-q-k)}\\sim F(q,n-q-k),$$\n",
    "\n",
    "where $q$ is the number of restricted parameter ($\\delta$s), $k$ number of $\\beta$s and $n$ number of observations. Higher values of F evidence agnist the null."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measurement errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measurement occurs when we assess a variable with inaccuracies, giving rise to two types of measurement errors: those pertaining to the dependent variable and those concerning the independent variables. This can lead to what's known as an omitted variable problem. Errors in measurement can stem from various sources, including inaccurate information provided by respondents or the inherent difficulty in quantifying certain aspects.\n",
    "### Measurement error in the dependent variable.\n",
    "\n",
    "\n",
    "OLS is sufficient when the measurement error is solely in the dependent variable. In this scenario, the measurement error becomes a component of the overall error term. As the magnitude of the error increases, so does the variance of the error term. It can established that, a higher variance of the error term results in a larger standard error. For instance, in a bivariate regression equation of the form$y_i=\\beta_0+\\beta_1x_i+\\varepsilon_i$, the variance of $\\beta_1$ isexpressed as $\\frac{\\sigma^2}{var(x_i^2)}$. Consequently, the variance of $\\beta_1$ will be larger when  $\\sigma^2$ is larger.\n",
    "### Measurement error in the independent variable. \n",
    "\n",
    "When measurement errors are present, the OLS estimate tends to systematically underestimate the coefficient's magnitude. To understand why, let's assume the true model is:\n",
    "$$y_i=\\beta_0+\\beta_1x^*_{1i}+\\varepsilon_i,$$\n",
    "where $x^*_{1i}$ is not directly observable and need to infer using $x_i$. We also assume the $E(\\varepsilon , x^*_{1i})$ to soley focus on mesurement error. let us assume that we observe $x_i$ that results from a combination of the true value $ x^*_{1i}$ and some error. For instance, consider the scenario where we observe reported savings rather than actual savings:\n",
    "$$x_{1i}=x_{1i}^*+v_i$$\n",
    "let us assume measurement error ($v_i$) to be disctributed with a mean of zero and exhibits no correlation with the true value. Now let us substitute the The above equation can be written as\n",
    "$$x_{1i}^*=x_{1i}-v_i$$\n",
    "Now let us substitute $x^*_{1i}$ for $x_{1i}$ in the equation of true model.\n",
    "$$\\begin{aligned}\n",
    "y_i&=\\beta_0+\\beta_1(x_{1i}-v_i)+\\varepsilon_i\\\\\n",
    "&=\\beta_0+\\beta_1x_{1i}+\\beta_1v_i+\\varepsilon_i\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe if we estimate $y_i=\\beta_0+\\beta_1x_{1i}$ then it is an omiited variable problem. Considering $v$ as the monnited variable and $-\\beta_1$ as the coefficient of on the ommitted variable we can express  bias in the $\\beta_1$ as follows.(Let us denote the $\\beta_1$ estimated by ommiting $v$ as $\\hat{\\beta}_1^{-v}$ for clarity.)\n",
    "$$E[\\hat{\\beta}_1^{-v}]=\\beta_1-\\beta_1 b_{1v},$$\n",
    "where $b_{1v}$ is the regression coefficient from a regression of $v$ on $x_{1.}$ following the formula omitted variable bias. Using the fact  $b_{1v}=\\frac{cov(x_1,v}{var(x_1)}$, we can the above equation as follows:\n",
    "$$E[\\hat{\\beta}_1^{-v}]=\\beta_1-\\beta_1 \\frac{cov(x_1,v)}{var(x_1)},$$\n",
    "Since $x_{1i}=x_{1i}^*+v_i$;\n",
    "$$\\begin{aligned}cov(x_{1i}v)&=cov(x_{1i}^*+vi, vi)\\\\\n",
    "&=cov(x_{1i}^*,vi)+cov(v_i, vi)\\\\\n",
    "&=cov(v,v), [cov(x_{1i}^*,vi) = 0 \\text{ since we assume } x_{1i}^*\\text{ and } v_i \\text{to be independent}]\\\\\n",
    "&=\\sigma^2_v\n",
    "\\end{aligned}$$\n",
    " and\n",
    " $$var(x_{1i})=\\sigma^2_{x^*}+\\sigma^2_v$$\n",
    " \n",
    " Using these results we can rewrite the bias as follows\n",
    " $$E[\\hat{\\beta}_1^{-v}]=\\beta_1-\\beta_1 \\frac{\\sigma^2_v}{\\sigma^2_{x^*}+\\sigma^2_v}$$  rerranaging the terms we get\n",
    " \n",
    " $$ E[\\hat{\\beta}_1^{-v}]=\\beta_1\\left(1- \\frac{\\sigma^2_v}{\\sigma^2_{x^*}+\\sigma^2_v}\\right)$$ it can be also be expressed as\n",
    "  $$ E[\\hat{\\beta}_1^{-v}]=\\beta_1\\left( \\frac{\\sigma^2_{x^*}}{\\sigma^2_{x^*}+\\sigma^2_v}\\right)$$ \n",
    "  \n",
    "Notice that $\\hat{\\beta_1}$, converges to the true coefficient($\\beta_1$) times a quantity that must be less than 1. In other words, as the sample size increases significantly, the estimated coefficient will converge to a value that is less than or equal to the true  $\\beta_1$.\n",
    "\n",
    "The equation becomes intuitive when we examine two extreme scenarios. If $\\sigma^2_v$  is zero, indicating no variance in the measurement error (as per our assumption of it being a mean-zero random variable), then $\\frac{\\sigma^2_{x^*}}{\\sigma^2_{x^*}+\\sigma^2_v}$ equals 1 (assuming $\\sigma^2_{x^*}$, is non-zero, implying $x^*_1$ varies). In essence, when there is no error in the measured value of$x_1$(which is what $\\sigma^2_v=0$ means), then $E[\\hat{\\beta}_1^{-v}]=\\beta_1$ and our estimate of $\\beta_1$ converges to the true value as the sample size grows. This aligns with intuition: no measurement error, no issue; OLS will yield an unbiased estimate.\n",
    "\n",
    "\n",
    "\n",
    "Conversely, if $\\sigma^2_v$, is significantly larger than $\\sigma^2_{x^*}$, indicating substantial variability in the measurement error compared to $x^*_1$, then $\\frac{\\sigma^2_{x^*}}{\\sigma^2_{x^*}+\\sigma^2_v}$ will be less than 1 and possibly close to zero, implying that the expectation of $\\hat{\\beta_1}$ will be smaller than the true value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We term this specific instance of omitted variable bias as __attenuation bias__ because when we exclude the measurement error term from the model, our estimate of $\\hat{\\beta}_1$ deviates from the true value by a multiplicative factor between zero and one. This implies that $\\hat{\\beta}_1$ will tend to be closer to zero than it should be when $x_i$ is measured with error.If the true value of ${\\beta}_1$ is positive, we observe $\\hat{\\beta}_1$ values less than they should be. Conversely, if the true  ${\\beta}_1$is negative, $\\hat{\\beta}_1$ values appear larger (closer to zero) than they should be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multicollinearity \n",
    "\n",
    "The problem of multicollinearity exits if the explanatory (independent) variables are highly correlated. If the relationship among the explanatory variables are exact and linear then we say there exits a perfect multicollinearity problem. In such cases the parameters of a model are not estimable separately. But more often the intercorrelations among explanatory variables are approximate in nature. In such cases the coefficients would be estimable but with less precision.\n",
    "\n",
    "### Consequences\n",
    "- If multicollinearity is perfect (exact linear relationship among explanatory ($x$) variables) then the regression coeffcients of the $x$ variables are indeterminate and their standard errors are infinite.\n",
    "- If multicollinearity is less than perfect (approximate linear relationship among the $x$ variables), then the regression coe¢ cients, although determinate, can possess large standard errors ( High standard error can happen due to other factors too), which means the coe¢ cients cannot be estimated with great precision or accuracy.\n",
    "\n",
    "#### Perfect multicollinearity\n",
    "Consider the following model $$y=\\beta _{0}+\\beta _{1}x_{1}+\\beta\n",
    "_{2}x_{2}+\\varepsilon. $$ If $x_{2}=2x_{1},$ we have\n",
    "\n",
    "$$y=\\beta _{0}+\\beta _{1}x_{1}+\\beta _{2}(2x_{1})+u=\\beta _{0}+(\\beta\n",
    "_{1}+2\\beta _{2})x_{1}+\\varepsilon.$$ Thus in this case $\\beta _{1}+2\\beta _{2}$\n",
    "estimable but we can not estimate $\\beta _{1}$ and $\\beta _{2}$ separately.\n",
    "\n",
    "#### High but imperfect multicollinearity\n",
    "In practice we rarely encounter perfect multicollinearity. In general there\n",
    "may exit an approximate linear relation among the independent variables\n",
    "rather than an exact linear as we discussed earlier. In such cases we can\n",
    "estimate the coefficients and if they satisfy all other ideal conditions the\n",
    "LS estimators will remain  unbiased estimators . However, the standard errors are technically \"correct\"\n",
    "but they will be very, very large. The intuition is, again, that the independent variables are not providing much independent information in the model and so our coefficients are not estimated with a lot of certainty.\n",
    "##### Residual interpretation of multiple regression estimates\n",
    "In order to get the idea let us first lookinto the residual interpretation of coeffients in a multi variable regression.\n",
    "Let\n",
    "$$y_i=\\beta_0+ \\beta_1x_{1i}+\\beta_2x_{2i}+\\varepsilon_i,$$\n",
    "Assuming that the idial condiation are valid we can interpret the coffiecificients as follows.\n",
    "$\\beta_1$ is the effect of $x_{1i}$ keeping $x_{2i}$ constant. Similarly, $\\beta_2$ is the effect of $x_{2i}$ keeping $x_{1i}$ contant. Take for insatance $\\beta_1$. This coefficient can be drived as follows \n",
    "1. Run the regression of $x_1$ on $x_2$. This is known as auxiliary regression of $x_1$. (regression of one independent variable on other independent variables) and obtain the residuals $\\hat{v}_1$ i.e.,$x_1=\\hat{\\alpha_0}+\\hat{\\alpha_1}x_2+\\hat{v}_1$\n",
    "2. Run the simple regression of $y$ on $\\hat{v}_1$, the resulting  estimate of the slope coefficient is $\\hat{\\beta}_1$.\n",
    "The first regression essentially cleans out the effect of the $x_2$ from $x_1$, leaving the variation\n",
    "unique to $x_1$ in $\\hat{ν}_1$.  Thus the equation for $\\hat{\\beta_1}$ can be interpreted as a simple linear regression coefficient of $y$ on this residual. This is in line with the partial derivative interpretation of $\\beta$s. Thus,\n",
    "$$\\hat{\\beta}_1=\\frac{\\sum_{i=1}^{n}\\hat{v}_{1i}(y_{i}-\\bar{y})}{\\sum_{i=1}^{n}\\hat{v}_{1i}^2}$$ and \n",
    "$$var(\\hat{\\beta}_1)=\\frac{\\sigma^2}{{\\sum_{i=1}^{n}\\hat{v}_{1i}^2}},$$\n",
    "remember that $\\bar{\\hat{v}}_1=0$. Also recll that\n",
    "$R^2=1-\\frac{SSR}{SST}$ for any regression. let $R^2_2$ be the $R_1^2$ for the regression of $x_1$ on $x_2$, then $R_1^2=1-\\frac{\\sum_{i=1}^{n}\\hat{v}_1^2}{\\sum_{i=1}^{n}(x_{1i}-\\bar{x_1})^2}$, here SST=${\\sum_{i=1}^{n}(x_{1i}-\\bar{x_1})^2}$ and SSR=${\\sum_{i=1}^{n}\\hat{v}_1^2}$ Thus using $R_1^2= 1-\\frac{SSR}{SST}$ we have $${\\sum_{i=1}^{n}\\hat{v}_1^2}={\\sum_{i=1}^{n}(x_{1i}-\\bar{x_1})^2}(1-R^2_1)$$ and substitutin gthis expression in the variance equation of $\\hat{\\beta}_1$ we have\n",
    "$$var(\\hat{\\beta}_1)=\\frac{\\sigma^2}{{\\sum_{i=1}^{n}(x_{1i}-\\bar{x_1})^2}(1-R^2_1)},$$\n",
    "here $R_1^2$ is the $R^2$ from the auxiliary regression of $x_1$ on $x_2$.\n",
    "\n",
    "Remember that $R^2$ is always between 0 and 1, so as $R_1^2$ gets bigger,$(1 - R^2_1)$ gets smaller, which in turn makes $var(\\hat{\\beta}_1)$ bigger. The intuition is that if variable $x_1$, is virtually indistinguishable from the other independent variables ($x_2$ in this case), it should in fact be hard to tell how much that variable affects $y$, and we will therefore have a larger $var(\\hat{\\beta}_1)$.\n",
    "\n",
    "In other words, when an independent variable is highly related to other independent variables, the variance of the coefficient we estimate for that variable will be high.\n",
    "\n",
    "The term $(1 - R^2_1)$is referred to as the __variance inflation factor (VIF)__. It measures how much variance is inflated owing to multicollinearity relative to a case in which there is no multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Reference:__\n",
    "These notes are prepared as teaching notes: Do not quote. The reference will be updated later."
   ]
  }
 ],
 "metadata": {
  "_draft": {
   "nbviewer_url": "https://gist.github.com/0541ed961e7190f52f02a45c2f1a3108"
  },
  "gist": {
   "data": {
    "description": "Note on Specification errors",
    "public": false
   },
   "id": "0541ed961e7190f52f02a45c2f1a3108"
  },
  "kernelspec": {
   "display_name": "R 4.2.2",
   "language": "R",
   "name": "ir42"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.2.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
